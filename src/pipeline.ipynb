{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-03 21:50:35.597598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-03 21:50:35.678299: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-11-03 21:50:35.678311: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-11-03 21:50:35.700399: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-11-03 21:50:36.143895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2022-11-03 21:50:36.143932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2022-11-03 21:50:36.143936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Embedding, LSTM\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os.path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiyC7HkqxlUD"
      },
      "source": [
        "## Read data using pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UEfJ8TcMpe-2"
      },
      "outputs": [],
      "source": [
        "PATH_LABLED = \"/home/erik/TU/ni/plagiate_labeltool/data/labled/\"\n",
        "PATH_TEMPLATE = \"/home/erik/TU/ni/plagiate_labeltool/data/code_templates/\"\n",
        "df1 = pd.read_csv(PATH_LABLED+'PPR [SoSe21]-9. Hausaufgabe - Pflichttest C-Antworten_plagiate.csv')\n",
        "df2 = pd.read_csv(PATH_LABLED+'PPR [SoSe21]-9. Hausaufgabe - Pflichttest C-Antworten_labled.csv')\n",
        "df3 = pd.read_csv(PATH_LABLED+'PPR [WS2021]-9. Hausaufgabe - Pflichttest C-Antworten_labled.csv')\n",
        "df3 = df3.drop('Unnamed: 0', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K873P-Pp8c7"
      },
      "source": [
        "This is what the data looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8FkK6QIRpjd4",
        "outputId": "c5765939-30fe-47ae-83aa-e2eea8d73398"
      },
      "outputs": [],
      "source": [
        "# df1.head(2)\n",
        "# df2.head(2)\n",
        "# df2.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove Given Code from Student Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_given_code(file):\n",
        "    try:\n",
        "        with open(file) as xmlstr:\n",
        "            soup = BeautifulSoup(xmlstr, 'xml')\n",
        "            answerpreload = soup.find('answerpreload').text\n",
        "            questiontext = soup.find('questiontext').text\n",
        "            return answerpreload, questiontext\n",
        "    except FileNotFoundError:\n",
        "        return \"Keine Vorgabedatei im Repo gefunden\", \"Keine Vorgabedatei im Repo gefunden\"\n",
        "\n",
        "\n",
        "def remove_given_code(code, preload_file_path):\n",
        "    answerpreload, _ = get_given_code(preload_file_path)\n",
        "    # remove empty lines\n",
        "    code = '\\n'.join([s for s in code.splitlines() if s.strip() != ''])\n",
        "    if answerpreload == 'Keine Vorgabedatei im Repo gefunden':\n",
        "        return code\n",
        "    # print(answerpreload)\n",
        "    answerpreload = answerpreload.replace('\\t', '').replace('\\r', '')\n",
        "    answerpreload = '\\n'.join([s for s in answerpreload.splitlines() if s.strip() != ''])\n",
        "    for ap_tmp in answerpreload.splitlines():\n",
        "        ap_tmp = re.escape(ap_tmp)\n",
        "        ap_tmp = '^' + ap_tmp + '$'\n",
        "        # {{ cr_random.f1 }} --> \\S*\n",
        "        ap_tmp = re.sub(\n",
        "            r\"\\\\{\\\\{\\\\\\s*\\S+\\s*\\\\}\\\\}\", r\"\\\\S*\", ap_tmp)\n",
        "        for code_tmp in code.splitlines():\n",
        "            # print((re.match(ap_tmp, code_tmp.replace('\\t', '').replace('\\r', ''))!=None, ap_tmp, code_tmp))\n",
        "            if re.match(ap_tmp, code_tmp.replace('\\t', '').replace('\\r', '')):\n",
        "                code = code.replace(code_tmp+'\\n', '', 1)\n",
        "                break\n",
        "    return code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concat the DataFrames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_list = [df1, df2]\n",
        "\n",
        "def remove_given_code_from_df(df_list):\n",
        "    for df in df_list:\n",
        "        semester = df[\"semester\"].values[0]\n",
        "        ha = df[\"ha\"].values[0]\n",
        "        prog_lang = df[\"prog_lang\"].values[0]\n",
        "        task = df[\"task\"].values[0]\n",
        "        answerpreload_path = f'{PATH_TEMPLATE}PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_lang}-Antworten_{task}.xml'\n",
        "        df['code1'] = [remove_given_code(code, answerpreload_path) for code in df['code1']]\n",
        "        df['code2'] = [remove_given_code(code, answerpreload_path) for code in df['code2']]\n",
        "\n",
        "remove_given_code_from_df(df_list)\n",
        "concat_df = pd.concat(df_list, ignore_index=True)\n",
        "concat_df = concat_df.drop('Unnamed: 0', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use WS2021 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prüfe ob code von Aufgabe a oder b ist und entferne die jeweilige vorgabe\n",
        "def remove_given_code_from_df_WS2021(df):\n",
        "    semester = df[\"semester\"].values[0]\n",
        "    ha = df[\"ha\"].values[0]\n",
        "    prog_lang = df[\"prog_lang\"].values[0]\n",
        "    task = df[\"task\"].values[0]\n",
        "    number = int(re.findall('(\\d+)', task)[0])\n",
        "    # es gibt ein ISIS bug, wo teilweise die Antwort zur vorherigen Frage in der nächsten Spalte steht\n",
        "    task_2 = re.sub('\\d+', str(number-1), task)\n",
        "    answerpreload_path = f'{PATH_TEMPLATE}PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_lang}-Antworten_{task}.xml'\n",
        "    alternative_answerpreload_path = f'{PATH_TEMPLATE}PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_lang}-Antworten_{task_2}.xml'\n",
        "    # möglich das mehr automatisiert zu machen?\n",
        "    ap = re.escape('char test[11]= \"0123456789\";')\n",
        "    df['code1'] = [remove_given_code(code, alternative_answerpreload_path) if re.findall(ap,code) else remove_given_code(code, answerpreload_path) for code in df['code1']]\n",
        "    df['code2'] = [remove_given_code(code, alternative_answerpreload_path) if re.findall(ap,code) else remove_given_code(code, answerpreload_path) for code in df['code2']]\n",
        "\n",
        "remove_given_code_from_df_WS2021(df3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split train and test data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load splited data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train = pd.read_csv(PATH_LABLED + 'train_data_HA9.csv',sep=',')\n",
        "data_val = pd.read_csv(PATH_LABLED + 'val_data_HA9.csv',sep=',')\n",
        "data_test= pd.read_csv(PATH_LABLED + 'test_data_HA9.csv',sep=',')\n",
        "X_train, y_train = data_train[['code1','code2']], data_train[['label']] \n",
        "X_val, y_val = data_val[['code1','code2']], data_val[['label']]\n",
        "X_test, y_test = data_test[['code1','code2']], data_test[['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create splited data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_temp, X_test, y_temp, y_test = train_test_split(concat_df[['code1', 'code2']], concat_df['label'], test_size=0.2, random_state=42)\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save splited data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pd.concat([X_train,y_train], axis=1).to_csv('../data/labled/train_data_HA9.csv',sep=',',index=False)\n",
        "# pd.concat([X_val,y_val], axis=1).to_csv('../data/labled/val_data_HA9.csv',sep=',',index=False)\n",
        "# pd.concat([X_test,y_test], axis=1).to_csv('../data/labled/test_data_HA9.csv',sep=',',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO remove newlines and ect\n",
        "def striphtml(data): \n",
        "    return re.sub('<.*?>', ' ', str(data)) \n",
        "\n",
        "def stripunc(data): \n",
        "    return re.sub('[^A-Za-z%\\._\\[\\]]+', ' ', str(data), flags=re.MULTILINE|re.DOTALL) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# csv_path = \"/home/erik/TU/ni/plagiate_labeltool/data/labled/PPR [WS2021]-9. Hausaufgabe - Pflichttest C-Antworten_labled.csv\"\n",
        "# df_test_remove = pd.read_csv(csv_path)\n",
        "\n",
        "# semester = 'WS2021'\n",
        "# ha = '9'\n",
        "# prog_language = 'C'\n",
        "# task = 'Antwort 9'\n",
        "# answerpreload_path = f'../data/code_templates/PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_language}-Antworten_{task}.xml'\n",
        "# # print(df_test_remove['code2'].values[2])\n",
        "# result = remove_given_code(df_test_remove['code2'].values[2], answerpreload_path)\n",
        "# # print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess the pairs for the models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_tokenizer(data, tokenizer): \n",
        "    data['code1_strip'] = data[['code1']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code2_strip'] = data[['code2']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code12'] = data[['code1_strip','code2_strip']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1)\n",
        "    tokenizer.fit_on_texts(data['code12'].values)\n",
        "    return tokenizer\n",
        "\n",
        "def preprocess_student_pairs_with_tokenizer(data, tokenizer): \n",
        "    # answerpreload_path = f'../data/code_templates/PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_language}-Antworten_{task}.xml'\n",
        "    # data['code1_strip'] = data[['code1']].apply(lambda x:remove_given_code(x[0]), axis=1)\n",
        "    # data['code2_strip'] = data[['code2']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code1_strip'] = data[['code1']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code2_strip'] = data[['code2']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code1_tokend'] = data['code1_strip'].apply(lambda x:tokenizer.texts_to_sequences([str(x)]))\n",
        "    data['code2_tokend'] = data['code2_strip'].apply(lambda x:tokenizer.texts_to_sequences([str(x)]))\n",
        "    data['code1_padded'] = data['code1_tokend'].apply(lambda x:pad_sequences(x, maxlen=256, padding='post', truncating='post'))\n",
        "    data['code2_padded'] = data['code2_tokend'].apply(lambda x:pad_sequences(x, maxlen=256, padding='post', truncating='post'))\n",
        "    data['code12_padded'] = [np.concatenate((x[0], x[1]), axis=None) for x in data[['code1_padded','code2_padded']].values]\n",
        "    x = [np.asarray(x).astype('int32') for x in data['code12_padded']]\n",
        "    x = tf.convert_to_tensor(x)\n",
        "    # x = tf.convert_to_tensor(x)\n",
        "    x_array = np.array(x)\n",
        "    return x_array\n",
        "\n",
        "def preprocess_student_pairs(data): \n",
        "    # answerpreload_path = f'{PATH_TEMPLATE}PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_language}-Antworten_{task}.xml'\n",
        "    # data['code1_strip'] = data[['code1']].apply(lambda x:remove_given_code(x[0]), axis=1)\n",
        "    # data['code2_strip'] = data[['code2']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code1_strip'] = data[['code1']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code2_strip'] = data[['code2']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    # x = [np.asarray(x).astype('int32') for x in data['code12_padded']]\n",
        "    # x = tf.convert_to_tensor(x)\n",
        "    # # x = tf.convert_to_tensor(x)\n",
        "    # x_array = np.array(x)\n",
        "    # return x_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load a Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_tokenizer(parent_folder):\n",
        "    with open(parent_folder + 'tokenizer.json') as file:\n",
        "        content = json.load(file)\n",
        "        return tokenizer_from_json(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-03 21:50:40.238395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-11-03 21:50:40.238667: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-11-03 21:50:40.238681: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (erik-MS-7C56): /proc/driver/nvidia/version does not exist\n",
            "2022-11-03 21:50:40.239897: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#TODO fit_on_text nur für x_train benutzen\n",
        "#TODO anderen algo benutzen\n",
        "#TODO Tokenizer -> TextVectorization\n",
        "\n",
        "# tokenizer = load_tokenizer('../data/model/trained_tokenizer/')\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer = train_tokenizer(X_train,tokenizer)\n",
        "\n",
        "# x_train = preprocess_student_pairs_with_tokenizer(X_train)\n",
        "# x_val = preprocess_student_pairs_with_tokenizer(X_val)\n",
        "# x_test = preprocess_student_pairs_with_tokenizer(X_test)\n",
        "preprocess_student_pairs(X_train)\n",
        "preprocess_student_pairs(X_val)\n",
        "preprocess_student_pairs(X_test)\n",
        "preprocess_student_pairs(df3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer.texts_to_sequences(['include stdio.h include stdlib.h typedef struct _String char str unsigned int str_len String String arguments int arg_count char args String arguments int arg_count char args String arguments malloc sizeof String int count arguments[ ].str_len arguments[ ].str malloc sizeof char for int i i arg_count i char arg args[i] int j char c arg[j] while c if c x c X c y c Y c q c Q j c arg[j] continue arguments[ ].str[count] c count j c arg[j] arguments[ ].str_len count arguments[ ].str_len arguments[ ].str malloc sizeof char for int i i arguments[ ].str_len i if i% arguments[ ].str[arguments[ ].str_len] arguments[ ].str[i] arguments[ ].str_len return arguments int main int argc char argv String str arguments argc argv printf x y q und X Y Q aussortiert %s nDer neue String lautet %s str[ ].str str[ ].str return   include stdio.h int main int argc char argv char array [ ] char array [ ] int k for int i i argc i for int j argv[i][j] j if argv[i][j] x argv[i][j] argv[i][j] y argv[i][j] q argv[i][j] X argv[i][j] Y argv[i][j] Q array[k] argv[i][j] k array[k] printf x y q und X Y Q aussortiert %s n array for int i i k i array [i] array[ i] printf Der neue String lautet %s array '])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save a Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_tokenizer(tokenizer,parent_folder):\n",
        "    with open(parent_folder + 'tokenizer.json', 'w', encoding='utf-8') as file:\n",
        "        file.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))\n",
        "\n",
        "# save_tokenizer(tokenizer,'../data/model/trained_tokenizer/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### My Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "B_fwhgU32iMt"
      },
      "outputs": [],
      "source": [
        "class simple_nn():\n",
        "    def __init__(self) -> None:\n",
        "        self.model = 0\n",
        "    \n",
        "    def create_simple_nn_model(self):\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.model.add(tf.keras.Input(shape=(256*2,), dtype='int32')),\n",
        "        self.model.add(tf.keras.layers.Dense(256*2, activation='relu')),\n",
        "        self.model.add(tf.keras.layers.Dense(256*2, activation='relu')),\n",
        "        self.model.add(tf.keras.layers.Dense(256*2, activation='relu')),\n",
        "        # self.model.add(tf.keras.layers.Dense(4, activation='relu')),\n",
        "        self.model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        # self.model.build(input_shape=(1,256))\n",
        "        # plot_model(self.model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "    def load_nn_model(self):\n",
        "        path_to_model = '../data/model/new_trained_simple_model'\n",
        "        self.model = load_model(path_to_model)\n",
        "\n",
        "    def save_nn_model(self):\n",
        "        path_to_model = '../data/model/new_trained_simple_model'\n",
        "        if os.path.isfile(path_to_model) is False:\n",
        "            self.model.save(path_to_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Utils for trained simple nn model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model = simple_nn()\n",
        "#Create our simple nn model\n",
        "# Model.create_simple_nn_model()\n",
        "#load simple_nn\n",
        "# Model.load_nn_model()\n",
        "#save simple_nn\n",
        "# Model.save_nn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(189, 512)\n",
            "(189, 1)\n",
            "(48, 512)\n",
            "(48, 1)\n"
          ]
        }
      ],
      "source": [
        "# print(x_train.shape)\n",
        "# print(y_train.shape)\n",
        "# print(x_val.shape)\n",
        "# print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 14.3402 - accuracy: 0.6085 - val_loss: 32.8354 - val_accuracy: 0.4792\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.3356 - accuracy: 0.9048 - val_loss: 11.0034 - val_accuracy: 0.3750\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.1202 - accuracy: 0.8942 - val_loss: 21.0224 - val_accuracy: 0.4583\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3099 - accuracy: 0.9630 - val_loss: 21.4256 - val_accuracy: 0.4792\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2408 - accuracy: 0.9683 - val_loss: 14.6315 - val_accuracy: 0.4792\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8e9c1a84c0>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train our simple nn model\n",
        "# Model.model.fit(x_train, y_train.values.reshape(-1,1), batch_size=32, epochs=5, validation_data=(x_val,y_val.values.reshape(-1,1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "189\n",
            "512\n",
            "object\n",
            "int32\n",
            "(None, 512) <dtype: 'int32'>\n",
            "(None, 1) <dtype: 'float32'>\n",
            "dense_4 (None, 512) float32\n",
            "dense_5 (None, 512) float32\n",
            "dense_6 (None, 512) float32\n",
            "dense_7 (None, 512) float32\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(X_train['code12_padded']))\n",
        "print(len(X_train['code12_padded'][0]))\n",
        "print(X_train['code12_padded'].dtype)\n",
        "print(X_train['code12_padded'][0].dtype)\n",
        "\n",
        "# def show_shapes(): # can make yours to take inputs; this'll use local variable values\n",
        "#     print(\"Expected: (num_samples, timesteps, channels)\")\n",
        "#     print(\"Sequences: {}\".format(Sequences.shape))\n",
        "#     print(\"Targets:   {}\".format(Targets.shape))\n",
        "\n",
        "[print(i.shape, i.dtype) for i in Model.model.inputs]\n",
        "[print(o.shape, o.dtype) for o in Model.model.outputs]\n",
        "[print(l.name, l.input_shape, l.dtype) for l in Model.model.layers]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 788,481\n",
            "Trainable params: 788,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Model.model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Evaluate my Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 2ms/step - loss: 13.7553 - accuracy: 0.3833\n",
            "2/2 [==============================] - 0s 1ms/step - loss: 14.6315 - accuracy: 0.4792\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[14.631500244140625, 0.4791666567325592]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model.model.evaluate(x_test, y_test.values)\n",
        "# Model.model.evaluate(x_val, y_val.values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My first Baseline Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Text Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "# model = tf.keras.Sequential([\n",
        "#   layers.Embedding(max_features + 1, embedding_dim),\n",
        "#   layers.Dropout(0.2),\n",
        "#   layers.GlobalAveragePooling1D(),\n",
        "#   layers.Dropout(0.2),\n",
        "#   layers.Dense(1)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Quora siames model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "# from tensorflow.keras.layers import (\n",
        "#     BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        "# )\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers import concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
        "from keras import backend as K\n",
        "\n",
        "def cosine_distance(vests):\n",
        "    x, y = vests\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "def cos_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO ersetzte word vector mit char dictionary\n",
        "\n",
        "# Create tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "\n",
        "def get_alphabet():\n",
        "    ''' Create alphabet from ASCII character '''\n",
        "    char_dict = {}\n",
        "    for num in range(127):\n",
        "        char_dict[chr(num)] = num + 1\n",
        "    return char_dict\n",
        "\n",
        "# Create Alphabet Vocabulary (Dictonary)\n",
        "char_dict = get_alphabet()\n",
        "# print(char_dict)\n",
        "\n",
        "# Create vocabulary and Add it into the tokenizer\n",
        "tk.word_index = char_dict.copy()\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# print(tk.word_index)\n",
        "\n",
        "vocab_size = len(tk.word_index)\n",
        "# print(vocab_size)\n",
        "\n",
        "# One hot array representation\n",
        "embedding_weights = []\n",
        "embedding_weights.append(np.zeros(vocab_size))\n",
        "for char, i in tk.word_index.items():\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i-1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "# print(embedding_weights.shape)\n",
        "# print(embedding_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# #https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "# embeddings_index = {}\n",
        "# f = open('/home/erik/Downloads/test_pretrained/glove.6B.300d.txt')\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# not_present_list = []\n",
        "# vocab_size = len(tokenizer.word_index) + 1\n",
        "# print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "# embedding_matrix = np.zeros((vocab_size, len(embeddings_index['no'])))\n",
        "# for word, i in tokenizer.word_index.items():\n",
        "#     if word in embeddings_index.keys():\n",
        "#         embedding_vector = embeddings_index.get(word)\n",
        "#     else:\n",
        "#         not_present_list.append(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#     else:\n",
        "#         embedding_matrix[i] = np.zeros(300)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save matrix\n",
        "print(type(embedding_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n",
            "128\n"
          ]
        }
      ],
      "source": [
        "# x_array\n",
        "print(x_train.shape[1])\n",
        "print(len(tk.word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_1 = Input(shape=(x_train.shape[1],))\n",
        "input_2 = Input(shape=(x_train.shape[1],))\n",
        "\n",
        "embedding_layer = Embedding(tk.word_index + 1, embedding_size, input_length=input_size, weights=[embedding_weights])\n",
        "\n",
        "common_embed = Embedding(name=\"synopsis_embedd\",input_dim =len(tk.word_index)+1, \n",
        "                       output_dim=len(tk.word_index),weights=[embedding_weights], \n",
        "                       input_length=x_train.shape[1],trainable=False) \n",
        "lstm_1 = common_embed(input_1)\n",
        "lstm_2 = common_embed(input_2)\n",
        "\n",
        "\n",
        "common_lstm = LSTM(64,return_sequences=True, activation=\"relu\")\n",
        "vector_1 = common_lstm(lstm_1)\n",
        "vector_1 = Flatten()(vector_1)\n",
        "\n",
        "vector_2 = common_lstm(lstm_2)\n",
        "vector_2 = Flatten()(vector_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "x3 = Subtract()([vector_1, vector_2])\n",
        "x3 = Multiply()([x3, x3])\n",
        "\n",
        "x1_ = Multiply()([vector_1, vector_1])\n",
        "x2_ = Multiply()([vector_2, vector_2])\n",
        "x4 = Subtract()([x1_, x2_])\n",
        "    \n",
        "    #https://stackoverflow.com/a/51003359/10650182\n",
        "x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([vector_1, vector_2])\n",
        "    \n",
        "conc = Concatenate(axis=-1)([x5, x4, x3])\n",
        "\n",
        "x = Dense(100, activation=\"relu\", name='conc_layer')(conc)\n",
        "x = Dropout(0.01)(x)\n",
        "out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n",
        "\n",
        "quora_model = Model([input_1, input_2], out)\n",
        "\n",
        "quora_model.compile(loss=\"binary_crossentropy\", metrics=['acc',auroc], optimizer=Adam(0.00001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quora_model.fit(x_train, y_train.values.reshape(-1,1), batch_size=32, epochs=5, validation_data=(x_val,y_val.values.reshape(-1,1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Reproduce the binary accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tf.keras.metrics.BinaryAccuracy()\n",
        "m = tf.keras.metrics.BinaryAccuracy()\n",
        "sum = 0\n",
        "zeros = 0\n",
        "ones = 0\n",
        "for p,l in zip(predict, label[9000:15000]):\n",
        "    m.update_state([[l]],[[p]])\n",
        "    print(f\"p: {p}; l: {l}; accuracy: {m.result().numpy()}\")\n",
        "    if m.result().numpy():\n",
        "        sum = sum + 1\n",
        "        if l:\n",
        "            ones = ones + 1\n",
        "        else:\n",
        "            zeros = zeros + 1\n",
        "    m.reset_state()\n",
        "print(f\"total accuracy: {sum/(15000-9000)}\")\n",
        "print(f\"number positive accuracy through zeros (round off): {zeros}\")\n",
        "print(f\"number positive accuracy through ones (round up): {ones}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## InifiniteMonkey\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# maybe https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
        "# def build_infinitemonkey_baseline():\n",
        "    # n grams\n",
        "    # tf-idf weighting\n",
        "    # cosine simularity\n",
        "    # grid search\n",
        "\n",
        "\n",
        "# def build_infinitemonkey_model():\n",
        "    # 2x encoder\n",
        "    # embedding (32)\n",
        "    # BatchNorm\n",
        "    # LSTM(128)\n",
        "    # 2x comparison module\n",
        "    # Concat the output of the two encoders as input\n",
        "    # BatchNorm\n",
        "    # Dense (128)\n",
        "    # ReLu\n",
        "    # 1x network\n",
        "    # Sum the output of the two comparison moduels as input\n",
        "    # BatchNorm\n",
        "    # Dense (128)\n",
        "    # ReLu\n",
        "    # BatchNorm\n",
        "    # Dense(1)\n",
        "    # Sigmoid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using matplotlib backend: TkAgg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# print(predict)\n",
        "# %matplotlib\n",
        "# %matplotlib inline\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize = (10,5))\n",
        "# plt.plot(predict)\n",
        "\n",
        "# plt.savefig('predictions_of_test_data.jpg')\n",
        "# step_size = np.arange(0.05,0.95,0.05)\n",
        "# step_size[1]\n",
        "# # plt.bar(step_size, np.round(predict,2))\n",
        "# for i in range(0,10):\n",
        "#     np.round(predict,2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Kopie von pandas_dataframe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('siames_network')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "9cc6df8cc6fa0c31d57213f9d666a037f059aeacd32b97b80ca2bbc4b39cefda"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
