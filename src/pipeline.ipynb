{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Embedding, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiyC7HkqxlUD"
      },
      "source": [
        "## Read data using pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UEfJ8TcMpe-2"
      },
      "outputs": [],
      "source": [
        "path1 = \"/home/erik/TU/ni/plagiate_labeltool/data/labled/PPR [SoSe21]-9. Hausaufgabe - Pflichttest C-Antworten_plagiate.csv\"\n",
        "path2 = \"/home/erik/TU/ni/plagiate_labeltool/data/labled/PPR [SoSe21]-9. Hausaufgabe - Pflichttest C-Antworten_labled.csv\"\n",
        "path3 = \"/home/erik/TU/ni/plagiate_labeltool/data/labled/PPR [WS2021]-9. Hausaufgabe - Pflichttest C-Antworten_labled.csv\"\n",
        "df1 = pd.read_csv(path1)\n",
        "df2 = pd.read_csv(path2)\n",
        "df3 = pd.read_csv(path3)\n",
        "df3 = df3.drop('Unnamed: 0', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K873P-Pp8c7"
      },
      "source": [
        "This is what the data looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8FkK6QIRpjd4",
        "outputId": "c5765939-30fe-47ae-83aa-e2eea8d73398"
      },
      "outputs": [],
      "source": [
        "# df1.head(2)\n",
        "# df2.head(2)\n",
        "# df2.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove Given Code from Student Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from re import T\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_given_code(file):\n",
        "    try:\n",
        "        with open(file) as xmlstr:\n",
        "            soup = BeautifulSoup(xmlstr, 'xml')\n",
        "            answerpreload = soup.find('answerpreload').text\n",
        "            questiontext = soup.find('questiontext').text\n",
        "            return answerpreload, questiontext\n",
        "    except FileNotFoundError:\n",
        "        return \"Keine Vorgabedatei im Repo gefunden\", \"Keine Vorgabedatei im Repo gefunden\"\n",
        "\n",
        "\n",
        "def remove_given_code(code, preload_file_path):\n",
        "    answerpreload, _ = get_given_code(preload_file_path)\n",
        "    # remove empty lines\n",
        "    code = '\\n'.join([s for s in code.splitlines() if s.strip() != ''])\n",
        "    if answerpreload == 'Keine Vorgabedatei im Repo gefunden':\n",
        "        return code\n",
        "    # print(answerpreload)\n",
        "    answerpreload = answerpreload.replace('\\t', '').replace('\\r', '')\n",
        "    answerpreload = '\\n'.join([s for s in answerpreload.splitlines() if s.strip() != ''])\n",
        "    for ap_tmp in answerpreload.splitlines():\n",
        "        ap_tmp = re.escape(ap_tmp)\n",
        "        ap_tmp = '^' + ap_tmp + '$'\n",
        "        # {{ cr_random.f1 }} --> \\S*\n",
        "        ap_tmp = re.sub(\n",
        "            r\"\\\\{\\\\{\\\\\\s*\\S+\\s*\\\\}\\\\}\", r\"\\\\S*\", ap_tmp)\n",
        "        for code_tmp in code.splitlines():\n",
        "            # print((re.match(ap_tmp, code_tmp.replace('\\t', '').replace('\\r', ''))!=None, ap_tmp, code_tmp))\n",
        "            if re.match(ap_tmp, code_tmp.replace('\\t', '').replace('\\r', '')):\n",
        "                code = code.replace(code_tmp+'\\n', '', 1)\n",
        "                break\n",
        "    return code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_list = [df1, df2]\n",
        "\n",
        "def remove_given_code_from_df(df_list):\n",
        "    for df in df_list:\n",
        "        semester = df[\"semester\"].values[0]\n",
        "        ha = df[\"ha\"].values[0]\n",
        "        prog_lang = df[\"prog_lang\"].values[0]\n",
        "        task = df[\"task\"].values[0]\n",
        "        answerpreload_path = f'../data/code_templates/PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_lang}-Antworten_{task}.xml'\n",
        "        df['code1'] = [remove_given_code(code, answerpreload_path) for code in df['code1']]\n",
        "        df['code2'] = [remove_given_code(code, answerpreload_path) for code in df['code2']]\n",
        "\n",
        "remove_given_code_from_df(df_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use WS2021 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prüfe ob code von Aufgabe a oder b ist und entferne die jeweilige vorgabe\n",
        "def remove_given_code_from_df_WS2021(df_list):\n",
        "    for df in df_list:\n",
        "        semester = df[\"semester\"].values[0]\n",
        "        ha = df[\"ha\"].values[0]\n",
        "        prog_lang = df[\"prog_lang\"].values[0]\n",
        "        task = df[\"task\"].values[0]\n",
        "        number = int(re.findall('(\\d+)', task)[0])\n",
        "        task_2 = re.sub('\\d+', str(number-1), task)\n",
        "        answerpreload_path = f'../data/code_templates/PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_lang}-Antworten_{task}.xml'\n",
        "        alternative_answerpreload_path = f'../data/code_templates/PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_lang}-Antworten_{task_2}.xml'\n",
        "        # möglich das mehr automatisiert zu machen?\n",
        "        ap = re.escape('char test[11]= \"0123456789\";')\n",
        "        df['code1'] = [remove_given_code(code, alternative_answerpreload_path) if re.findall(ap,code) else remove_given_code(code, answerpreload_path) for code in df['code1']]\n",
        "        df['code2'] = [remove_given_code(code, alternative_answerpreload_path) if re.findall(ap,code) else remove_given_code(code, answerpreload_path) for code in df['code2']]\n",
        "\n",
        "remove_given_code_from_df_WS2021([df3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concat the DataFrames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_list = [df1, df2]\n",
        "concat_df = pd.concat(df_list, ignore_index=True)\n",
        "concat_df = concat_df.drop('Unnamed: 0', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split train and test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_temp, X_test, y_temp, y_test = train_test_split(concat_df[['code1', 'code2']], concat_df['label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO remove newlines and ect\n",
        "def striphtml(data): \n",
        "    return re.sub('<.*?>', ' ', str(data)) \n",
        "\n",
        "def stripunc(data): \n",
        "    return re.sub('[^A-Za-z%\\._\\[\\]]+', ' ', str(data), flags=re.MULTILINE|re.DOTALL) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# csv_path = \"/home/erik/TU/ni/plagiate_labeltool/data/labled/PPR [WS2021]-9. Hausaufgabe - Pflichttest C-Antworten_labled.csv\"\n",
        "# df_test_remove = pd.read_csv(csv_path)\n",
        "\n",
        "# semester = 'WS2021'\n",
        "# ha = '9'\n",
        "# prog_language = 'C'\n",
        "# task = 'Antwort 9'\n",
        "# answerpreload_path = f'../data/code_templates/PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_language}-Antworten_{task}.xml'\n",
        "# # print(df_test_remove['code2'].values[2])\n",
        "# result = remove_given_code(df_test_remove['code2'].values[2], answerpreload_path)\n",
        "# # print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess the pairs for the models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_student_pairs(data, tokenizer): \n",
        "    # answerpreload_path = f'../data/code_templates/PPR [{semester}]-{ha}. Hausaufgabe - Pflichttest {prog_language}-Antworten_{task}.xml'\n",
        "    # data['code1_strip'] = data[['code1']].apply(lambda x:remove_given_code(x[0]), axis=1)\n",
        "    # data['code2_strip'] = data[['code2']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code1_strip'] = data[['code1']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code2_strip'] = data[['code2']].apply(lambda x:striphtml(stripunc((x[0]))), axis=1)\n",
        "    data['code12'] = data[['code1_strip','code2_strip']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1)\n",
        "    tokenizer.fit_on_texts(data['code12'].values)\n",
        "    data['code1_tokend'] = data['code1_strip'].apply(lambda x:tokenizer.texts_to_sequences([str(x)]))\n",
        "    data['code2_tokend'] = data['code2_strip'].apply(lambda x:tokenizer.texts_to_sequences([str(x)]))\n",
        "    data['code1_padded'] = data['code1_tokend'].apply(lambda x:pad_sequences(x, maxlen=256, padding='post', truncating='post'))\n",
        "    data['code2_padded'] = data['code2_tokend'].apply(lambda x:pad_sequences(x, maxlen=256, padding='post', truncating='post'))\n",
        "    import numpy as np\n",
        "    data['code12_padded'] = [np.concatenate((x[0], x[1]), axis=None) for x in data[['code1_padded','code2_padded']].values]\n",
        "    x = [np.asarray(x).astype('int32') for x in data['code12_padded']]\n",
        "    x = tf.convert_to_tensor(x)\n",
        "    # x = tf.convert_to_tensor(x)\n",
        "    x_array = np.array(x)\n",
        "    return x_array, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-29 14:09:07.653740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-10-29 14:09:07.654012: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-10-29 14:09:07.654025: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (erik-MS-7C56): /proc/driver/nvidia/version does not exist\n",
            "2022-10-29 14:09:07.655130: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "#TODO fit_on_text nur für x_train benutzen\n",
        "#TODO tokenizer einmal machen und dann speichern und laden\n",
        "#TODO anderen algo benutzen\n",
        "tokenizer = Tokenizer()\n",
        "x_val,tokenizer = preprocess_student_pairs(X_val,tokenizer)\n",
        "x_train,tokenizer = preprocess_student_pairs(X_train,tokenizer)\n",
        "x_test,tokenizer = preprocess_student_pairs(X_test,tokenizer)\n",
        "X_test_df3 = preprocess_student_pairs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### My Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "B_fwhgU32iMt"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(256*2,), dtype='int32')),\n",
        "model.add(tf.keras.layers.Dense(256*2, activation='relu')),\n",
        "model.add(tf.keras.layers.Dense(256*2, activation='relu')),\n",
        "model.add(tf.keras.layers.Dense(256*2, activation='relu')),\n",
        "# model.add(tf.keras.layers.Dense(4, activation='relu')),\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.build(input_shape=(1,256))\n",
        "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(189, 512)\n",
            "(189,)\n",
            "(48, 512)\n",
            "(48,)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 1.2924 - accuracy: 0.8307 - val_loss: 3.2557 - val_accuracy: 0.8333\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.9831 - accuracy: 0.8995 - val_loss: 3.4385 - val_accuracy: 0.9375\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3927 - accuracy: 0.9683 - val_loss: 0.6897 - val_accuracy: 0.9583\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2776 - accuracy: 0.9471 - val_loss: 0.3575 - val_accuracy: 0.9792\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1351 - accuracy: 0.9683 - val_loss: 0.9293 - val_accuracy: 0.9792\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa01c37af80>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train.values.reshape(-1,1), batch_size=32, epochs=5, validation_data=(x_val,y_val.values.reshape(-1,1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(X_train['code12_padded']))\n",
        "print(len(X_train['code12_padded'][0]))\n",
        "print(X_train['code12_padded'].dtype)\n",
        "print(X_train['code12_padded'][0].dtype)\n",
        "\n",
        "# def show_shapes(): # can make yours to take inputs; this'll use local variable values\n",
        "#     print(\"Expected: (num_samples, timesteps, channels)\")\n",
        "#     print(\"Sequences: {}\".format(Sequences.shape))\n",
        "#     print(\"Targets:   {}\".format(Targets.shape))\n",
        "\n",
        "[print(i.shape, i.dtype) for i in model.inputs]\n",
        "[print(o.shape, o.dtype) for o in model.outputs]\n",
        "[print(l.name, l.input_shape, l.dtype) for l in model.layers]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 788,481\n",
            "Trainable params: 788,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Evaluate my Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 2ms/step - loss: 1.1167 - accuracy: 0.8973\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 0.3189 - accuracy: 0.9844\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.31894052028656006, 0.984375]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(x_test, y_test.values)\n",
        "model.evaluate(x_val, y_val.values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "188/188 [==============================] - 0s 944us/step - loss: 3.9816 - accuracy: 0.6167\n",
            "188/188 [==============================] - 0s 861us/step\n"
          ]
        }
      ],
      "source": [
        "# with open('model_summary.txt', mode='w') as file:\n",
        "#     model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "# numeric_dataset = tf.data.Dataset.from_tensor_slices((new_df, label))\n",
        "\n",
        "# predict = model.predict(new_df[9000:15000])\n",
        "\n",
        "# evaluate 64_hand_labled_pairs\n",
        "# plt.show()\n",
        "\n",
        "# print(label)\n",
        "# zero = 0\n",
        "# one = 0\n",
        "# for row in label.values[:9000]:\n",
        "#     if row == 0:\n",
        "#         zero = zero +1\n",
        "#     elif row == 1:\n",
        "#         one = one +1\n",
        "# print(f'training_data: zeros = {zero}; ones = {one}')\n",
        "# zero = 0\n",
        "# one = 0\n",
        "# for row in label.values[9000:10000]:\n",
        "#     if row == 0:\n",
        "#         zero = zero +1\n",
        "#     elif row == 1:\n",
        "#         one = one +1\n",
        "# print(f'test_data: zeros = {zero}; ones = {one}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My first Baseline Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Text Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Quora siames model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "# from tensorflow.keras.layers import (\n",
        "#     BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        "# )\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
        "from keras import backend as K\n",
        "\n",
        "def cosine_distance(vests):\n",
        "    x, y = vests\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "def cos_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_present_list = []\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "embedding_matrix = np.zeros((vocab_size, len(embeddings_index['no'])))\n",
        "for word, i in t.word_index.items():\n",
        "    if word in embeddings_index.keys():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    else:\n",
        "        not_present_list.append(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        embedding_matrix[i] = np.zeros(300)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# x_array\n",
        "len(y_train.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Input' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/erik/TU/ni/plagiarism_detection_tool/src/my_jupiter_notebook_with_question_pairs.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/erik/TU/ni/plagiarism_detection_tool/src/my_jupiter_notebook_with_question_pairs.ipynb#ch0000123?line=0'>1</a>\u001b[0m input_1 \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(training_text1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erik/TU/ni/plagiarism_detection_tool/src/my_jupiter_notebook_with_question_pairs.ipynb#ch0000123?line=1'>2</a>\u001b[0m input_2 \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(training_text2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erik/TU/ni/plagiarism_detection_tool/src/my_jupiter_notebook_with_question_pairs.ipynb#ch0000123?line=4'>5</a>\u001b[0m common_embed \u001b[39m=\u001b[39m Embedding(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msynopsis_embedd\u001b[39m\u001b[39m\"\u001b[39m,input_dim \u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(t\u001b[39m.\u001b[39mword_index)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erik/TU/ni/plagiarism_detection_tool/src/my_jupiter_notebook_with_question_pairs.ipynb#ch0000123?line=5'>6</a>\u001b[0m                        output_dim\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(embeddings_index[\u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m]),weights\u001b[39m=\u001b[39m[embedding_matrix], \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erik/TU/ni/plagiarism_detection_tool/src/my_jupiter_notebook_with_question_pairs.ipynb#ch0000123?line=6'>7</a>\u001b[0m                        input_length\u001b[39m=\u001b[39mtrain_q1_seq\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \n",
            "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
          ]
        }
      ],
      "source": [
        "input_1 = Input(shape=(x_array.shape[1],))\n",
        "input_2 = Input(shape=(x_array.shape[1],))\n",
        "\n",
        "\n",
        "common_embed = Embedding(name=\"synopsis_embedd\",input_dim =len(t.word_index)+1, \n",
        "                       output_dim=len(embeddings_index['no']),weights=[embedding_matrix], \n",
        "                       input_length=training_text1.shape[1],trainable=False) \n",
        "lstm_1 = common_embed(input_1)\n",
        "lstm_2 = common_embed(input_2)\n",
        "\n",
        "\n",
        "common_lstm = LSTM(64,return_sequences=True, activation=\"relu\")\n",
        "vector_1 = common_lstm(lstm_1)\n",
        "vector_1 = Flatten()(vector_1)\n",
        "\n",
        "vector_2 = common_lstm(lstm_2)\n",
        "vector_2 = Flatten()(vector_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "x3 = Subtract()([vector_1, vector_2])\n",
        "x3 = Multiply()([x3, x3])\n",
        "\n",
        "x1_ = Multiply()([vector_1, vector_1])\n",
        "x2_ = Multiply()([vector_2, vector_2])\n",
        "x4 = Subtract()([x1_, x2_])\n",
        "    \n",
        "    #https://stackoverflow.com/a/51003359/10650182\n",
        "x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([vector_1, vector_2])\n",
        "    \n",
        "conc = Concatenate(axis=-1)([x5,x4, x3])\n",
        "\n",
        "x = Dense(100, activation=\"relu\", name='conc_layer')(conc)\n",
        "x = Dropout(0.01)(x)\n",
        "out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n",
        "\n",
        "quora_model = Model([input_1, input_2], out)\n",
        "\n",
        "quora_model.compile(loss=\"binary_crossentropy\", metrics=['acc',auroc], optimizer=Adam(0.00001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Reproduce the binary accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tf.keras.metrics.BinaryAccuracy()\n",
        "m = tf.keras.metrics.BinaryAccuracy()\n",
        "sum = 0\n",
        "zeros = 0\n",
        "ones = 0\n",
        "for p,l in zip(predict, label[9000:15000]):\n",
        "    m.update_state([[l]],[[p]])\n",
        "    print(f\"p: {p}; l: {l}; accuracy: {m.result().numpy()}\")\n",
        "    if m.result().numpy():\n",
        "        sum = sum + 1\n",
        "        if l:\n",
        "            ones = ones + 1\n",
        "        else:\n",
        "            zeros = zeros + 1\n",
        "    m.reset_state()\n",
        "print(f\"total accuracy: {sum/(15000-9000)}\")\n",
        "print(f\"number positive accuracy through zeros (round off): {zeros}\")\n",
        "print(f\"number positive accuracy through ones (round up): {ones}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## InifiniteMonkey\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def build_infinitemonkey_baseline():\n",
        "    # n grams\n",
        "    # tf-idf weighting\n",
        "    # cosine simularity\n",
        "    # grid search\n",
        "\n",
        "\n",
        "# def build_infinitemonkey_model():\n",
        "    # 2x encoder\n",
        "    # embedding (32)\n",
        "    # BatchNorm\n",
        "    # LSTM(128)\n",
        "    # 2x comparison module\n",
        "    # Concat the output of the two encoders as input\n",
        "    # BatchNorm\n",
        "    # Dense (128)\n",
        "    # ReLu\n",
        "    # 1x network\n",
        "    # Sum the output of the two comparison moduels as input\n",
        "    # BatchNorm\n",
        "    # Dense (128)\n",
        "    # ReLu\n",
        "    # BatchNorm\n",
        "    # Dense(1)\n",
        "    # Sigmoid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using matplotlib backend: TkAgg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# print(predict)\n",
        "%matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.plot(predict)\n",
        "# plt.savefig('predictions_of_test_data.jpg')\n",
        "# step_size = np.arange(0.05,0.95,0.05)\n",
        "# step_size[1]\n",
        "# # plt.bar(step_size, np.round(predict,2))\n",
        "# for i in range(0,10):\n",
        "#     np.round(predict,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVF7_Z-Mp-qD"
      },
      "source": [
        "### Save/Load our trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../data/model/new_trained_simple_model/assets\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "path_to_model = '../data/model/new_trained_simple_model'\n",
        "if os.path.isfile(path_to_model) is False:\n",
        "    model.save(path_to_model)\n",
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model(path_to_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Kopie von pandas_dataframe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('siames_network')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "9cc6df8cc6fa0c31d57213f9d666a037f059aeacd32b97b80ca2bbc4b39cefda"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
